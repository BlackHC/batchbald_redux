{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp joint_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joint_entropy Module\n",
    "> Compute joint entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module to help compute joint entropies for dependent categorical variables given via a density $p((y_i)_i|w))$ in the Bayesian setting. We compute the density $p((y_i)_i)$ by marginalizing over $w$.\n",
    "\n",
    "Two classes are provided:\n",
    "\n",
    "* exact computation (which works for up 5 to joint variables);\n",
    "* estimate using importance sampling of configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "import torch\n",
    "from toma import toma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run tests, we need a few distributions to run tests with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3 , 0.2 , 0.15, 0.35])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_mixture_prob_dist(p1, p2, m):\n",
    "  return (1. - m) * np.asarray(p1) + m * np.asarray(p2)\n",
    "\n",
    "p1=[0.1,0.2,0.2,0.5]\n",
    "p2=[0.5,0.2,0.1,0.2]\n",
    "get_mixture_prob_dist(p1,p2,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of inference samples `K`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[0.1,0.2,0.2,0.5]\n",
    "p2=[0.5,0.2,0.1,0.2]\n",
    "y1_ws=[get_mixture_prob_dist(p1,p2,m) for m in np.linspace(0, 1, K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[0.1,0.6,0.2,0.1]\n",
    "p2=[0.0,0.5,0.5,0.0]\n",
    "y2_ws=[get_mixture_prob_dist(p1,p2,m) for m in np.linspace(0, 1, K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_to_tensor(*l):\n",
    "    return torch.stack(list(map(torch.as_tensor, l))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_ws=nested_to_tensor(y1_ws, y2_ws, y1_ws,y2_ws, y1_ws,y2_ws, y1_ws,y2_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JointEntropy interface\n",
    "\n",
    "Before we look at any implementations, we want to define an interface that we want to support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class JointEntropy:\n",
    "    \"\"\"Random variables (all with the same # of categories $C$) can be added via `JointEntropy.add_variables`.\n",
    "\n",
    "`JointEntropy.compute` computes the joint entropy.\n",
    "\n",
    "`JointEntropy.compute_batch` computes the joint entropy of the added variables with each of the variables in the provided batch probabilities in turn.\"\"\"\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Computes the entropy of this joint entropy.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_variables(self, probs_N_K_C: torch.Tensor) -> 'JointEntropy':\n",
    "        \"\"\"Expands the joint entropy to include more terms.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_batch(self,\n",
    "                      probs_B_K_C: torch.Tensor,\n",
    "                      output_entropies_B=None) -> torch.Tensor:\n",
    "        \"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.add_variables\" class=\"doc_header\"><code>JointEntropy.add_variables</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.add_variables</code>(**`probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.compute\" class=\"doc_header\"><code>JointEntropy.compute</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.compute_batch\" class=\"doc_header\"><code>JointEntropy.compute_batch</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.compute_batch</code>(**`probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(JointEntropy.add_variables)\n",
    "show_doc(JointEntropy.compute)\n",
    "show_doc(JointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing exact joint entropies\n",
    "\n",
    "To compute exact joint entropies, we have to compute all possible configurations of the $y_i$ and evaluate $p(y_1, \\dots, y_n)$ by averaging over $p(y_1, \\dots, y_n|w)$.\n",
    "\n",
    "The number of samples $M=C^N$, where $N$ is the number of variables in the joint and $C$ is the number of classes.\n",
    "\n",
    "For this, we provide a class `ExactJointEntropy` that takes $K$ and starts with no variables in the joint.\n",
    "\n",
    "### Version in the paper\n",
    "![Version in the paper](batchbald_exact_joint_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports    \n",
    "\n",
    "class ExactJointEntropy(JointEntropy):\n",
    "    joint_probs_M_K: torch.Tensor\n",
    "\n",
    "    def __init__(self, joint_probs_M_K: torch.Tensor):\n",
    "        self.joint_probs_M_K = joint_probs_M_K\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(K: int, device=None, dtype=None) -> 'ExactJointEntropy':\n",
    "        return ExactJointEntropy(torch.ones((1, K), device=device,\n",
    "                                            dtype=dtype))\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        probs_M = torch.mean(self.joint_probs_M_K, dim=1, keepdim=False)\n",
    "        nats_M = -torch.log(probs_M) * probs_M\n",
    "        entropy = torch.sum(nats_M)\n",
    "        return entropy\n",
    "\n",
    "    def add_variables(self, probs_N_K_C: torch.Tensor) -> 'ExactJointEntropy':\n",
    "        assert self.joint_probs_M_K.shape[1] == probs_N_K_C.shape[1]\n",
    "\n",
    "        N, K, C = probs_N_K_C.shape\n",
    "        joint_probs_K_M_1 = self.joint_probs_M_K.t()[:, :, None]\n",
    "\n",
    "        # Using lots of memory.\n",
    "        for i in range(N):\n",
    "            probs_i__K_1_C = probs_N_K_C[i][:,\n",
    "                                            None, :].to(joint_probs_K_M_1,\n",
    "                                                        non_blocking=True)\n",
    "            joint_probs_K_M_C = joint_probs_K_M_1 * probs_i__K_1_C\n",
    "            joint_probs_K_M_1 = joint_probs_K_M_C.reshape((K, -1, 1))\n",
    "\n",
    "        self.joint_probs_M_K = joint_probs_K_M_1.squeeze(2).t()\n",
    "        return self\n",
    "\n",
    "    def compute_batch(self, probs_B_K_C: torch.Tensor,\n",
    "                            output_entropies_B=None):\n",
    "        assert self.joint_probs_M_K.shape[1] == probs_B_K_C.shape[1]\n",
    "        \n",
    "        B, K, C = probs_B_K_C.shape\n",
    "        M = self.joint_probs_M_K.shape[0]\n",
    "        \n",
    "        if output_entropies_B is None:\n",
    "            output_entropies_B = torch.empty(B, dtype=probs_B_K_C.dtype, device=probs_B_K_C.device)\n",
    "\n",
    "        @toma.execute.chunked(probs_B_K_C,\n",
    "                              initial_step=1024,\n",
    "                              dimension=0,\n",
    "                              context=\"ExactJointEntropy.batch_joint_entropy\")\n",
    "        def chunked_joint_entropy(chunked_probs_b_K_C: torch.Tensor,\n",
    "                                  start: int, end: int):\n",
    "            b = chunked_probs_b_K_C.shape[0]\n",
    "\n",
    "            probs_b_M_C = torch.empty((b, M, C),\n",
    "                                      dtype=self.joint_probs_M_K.dtype,\n",
    "                                      device=self.joint_probs_M_K.device)\n",
    "            for i in range(b):\n",
    "                torch.matmul(self.joint_probs_M_K,\n",
    "                             probs_B_K_C[i].to(self.joint_probs_M_K,\n",
    "                                               non_blocking=True),\n",
    "                             out=probs_b_M_C[i])\n",
    "            probs_b_M_C /= K\n",
    "\n",
    "            output_entropies_B[start:end].copy_(torch.sum(\n",
    "                -torch.log(probs_b_M_C) * probs_b_M_C, dim=(1, 2)),\n",
    "                                                non_blocking=True)\n",
    "            \n",
    "        return output_entropies_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.empty\" class=\"doc_header\"><code>ExactJointEntropy.empty</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.empty</code>(**`K`**:`int`, **`device`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.add_variables\" class=\"doc_header\"><code>ExactJointEntropy.add_variables</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.add_variables</code>(**`probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.compute\" class=\"doc_header\"><code>ExactJointEntropy.compute</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.compute_batch\" class=\"doc_header\"><code>ExactJointEntropy.compute_batch</code><a href=\"__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.compute_batch</code>(**`probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ExactJointEntropy.empty)\n",
    "show_doc(ExactJointEntropy.add_variables)\n",
    "show_doc(ExactJointEntropy.compute)\n",
    "show_doc(ExactJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479, dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4]).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.float)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4]).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.float)\n",
    "entropies =joint_entropy.add_variables(ys_ws[:4]).compute_batch(ys_ws)\n",
    "assert np.allclose(entropies, [5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362])\n",
    "entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing  approximate joint entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute approximate joint entropies, we have to sample possible configurations of the $y_i$ from $p(y_1, \\dots, y_n|w)$ stratified by $p(w)$ and evaluate $p(y_1, \\dots, y_n)$ by averaging over $p(y_1, \\dots, y_n|w)$.\n",
    "\n",
    "The number of samples is $M$, so we use $\\frac{M}{K}$ samples per $w$.\n",
    "\n",
    "For this, we provide a class `SampledJointEntropy` that takes $K$ and $M$, and implements the 'JointEntropy' interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample, we need a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def batch_multi_choices(probs_b_C, M: int):\n",
    "    \"\"\"\n",
    "    probs_b_C: Ni... x C\n",
    "\n",
    "    Returns:\n",
    "        choices: Ni... x M\n",
    "    \"\"\"\n",
    "    probs_B_C = probs_b_C.reshape((-1, probs_b_C.shape[-1]))\n",
    "\n",
    "    # samples: Ni... x draw_per_xx\n",
    "    choices = torch.multinomial(probs_B_C, num_samples=M, replacement=True)\n",
    "\n",
    "    choices_b_M = choices.reshape(list(probs_b_C.shape[:-1]) + [M])\n",
    "    return choices_b_M\n",
    "\n",
    "\n",
    "def gather_expand(data, dim, index):\n",
    "    if gather_expand.DEBUG_CHECKS:\n",
    "        assert len(data.shape) == len(index.shape)\n",
    "        assert all(dr == ir or 1 in (dr, ir) for dr, ir in zip(data.shape, index.shape))\n",
    "\n",
    "    max_shape = [max(dr, ir) for dr, ir in zip(data.shape, index.shape)]\n",
    "    new_data_shape = list(max_shape)\n",
    "    new_data_shape[dim] = data.shape[dim]\n",
    "\n",
    "    new_index_shape = list(max_shape)\n",
    "    new_index_shape[dim] = index.shape[dim]\n",
    "\n",
    "    data = data.expand(new_data_shape)\n",
    "    index = index.expand(new_index_shape)\n",
    "\n",
    "    return torch.gather(data, dim, index)\n",
    "\n",
    "\n",
    "gather_expand.DEBUG_CHECKS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version in the paper\n",
    "![Version in the paper](batchbald_importance_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class SampledJointEntropy(JointEntropy):\n",
    "    \"\"\"Random variables (all with the same # of categories $C$) can be added via `SampledJointEntropy.add_variables`.\n",
    "\n",
    "`SampledJointEntropy.compute` computes the joint entropy.\n",
    "\n",
    "`SampledJointEntropy.compute_batch` computes the joint entropy of the added variables with each of the variables in the provided batch probabilities in turn.\"\"\"\n",
    "    sampled_joint_probs_M_K: torch.Tensor\n",
    "\n",
    "    def __init__(self, sampled_joint_probs_M_K: torch.Tensor):\n",
    "        self.sampled_joint_probs_M_K = sampled_joint_probs_M_K\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(K: int, device=None, dtype=None) -> 'SampledJointEntropy':\n",
    "        return SampledJointEntropy(torch.ones((1, K), device=device,\n",
    "                                            dtype=dtype))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample(probs_N_K_C: torch.Tensor, M:int) -> 'SampledJointEntropy':\n",
    "        K = probs_N_K_C.shape[1]\n",
    "        \n",
    "        # S: num of samples per w\n",
    "        S = M // K\n",
    "\n",
    "        choices_N_K_S = batch_multi_choices(probs_N_K_C, S).long()\n",
    "\n",
    "        expanded_choices_N_1_K_S = choices_N_K_S[:, None, :, :]\n",
    "        expanded_probs_N_K_1_C = probs_N_K_C[:, :, None, :]\n",
    "\n",
    "        probs_N_K_K_S = gather_expand(expanded_probs_N_K_1_C, dim=-1, index=expanded_choices_N_1_K_S)\n",
    "        # exp sum log seems necessary to avoid 0s?\n",
    "        probs_K_K_S = torch.exp(torch.sum(torch.log(probs_N_K_K_S), dim=0, keepdim=False))\n",
    "        samples_K_M = probs_K_K_S.reshape((K, -1))\n",
    "\n",
    "        samples_M_K = samples_K_M.t()\n",
    "        return SampledJointEntropy(samples_M_K)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        sampled_joint_probs_M = torch.mean(self.sampled_joint_probs_M_K, dim=1, keepdim=False)\n",
    "        nats_M = -torch.log(sampled_joint_probs_M)\n",
    "        entropy = torch.mean(nats_M)\n",
    "        return entropy\n",
    "\n",
    "    def add_variables(self, probs_N_K_C: torch.Tensor, M2: int) -> 'SampledJointEntropy':\n",
    "        assert self.sampled_joint_probs_M_K.shape[1] == probs_N_K_C.shape[1]\n",
    "                  \n",
    "        sample_K_M1_1 = self.sampled_joint_probs_M_K.t()[:, :, None]\n",
    "        \n",
    "        new_sample_M2_K = self.sample(probs_N_K_C, M2).sampled_joint_probs_M_K\n",
    "        new_sample_K_1_M2 = new_sample_M2_K.t()[:, None, :]\n",
    "        \n",
    "        merged_sample_K_M1_M2 = sample_K_M1_1 * new_sample_K_1_M2\n",
    "        merged_sample_K_M = merged_sample_K_M1_M2.reshape((K, -1))\n",
    "        \n",
    "        self.sampled_joint_probs_M_K = merged_sample_K_M.t()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def compute_batch(self, probs_B_K_C: torch.Tensor,\n",
    "                            output_entropies_B=None):\n",
    "        assert self.sampled_joint_probs_M_K.shape[1] == probs_B_K_C.shape[1]\n",
    "        \n",
    "        B, K, C = probs_B_K_C.shape\n",
    "        M = self.sampled_joint_probs_M_K.shape[0]\n",
    "        \n",
    "        if output_entropies_B is None:\n",
    "            output_entropies_B = torch.empty(B, dtype=probs_B_K_C.dtype, device=probs_B_K_C.device)\n",
    "\n",
    "        @toma.execute.chunked(probs_B_K_C,\n",
    "                              initial_step=1024,\n",
    "                              dimension=0,\n",
    "                              context=\"ExactJointEntropy.batch_joint_entropy\")\n",
    "        def chunked_joint_entropy(chunked_probs_b_K_C: torch.Tensor,\n",
    "                                  start: int, end: int):\n",
    "            b = chunked_probs_b_K_C.shape[0]\n",
    "\n",
    "            probs_b_M_C = torch.empty((b, M, C),\n",
    "                                      dtype=self.sampled_joint_probs_M_K.dtype,\n",
    "                                      device=self.sampled_joint_probs_M_K.device)\n",
    "            for i in range(b):\n",
    "                torch.matmul(self.sampled_joint_probs_M_K,\n",
    "                             probs_B_K_C[i].to(self.sampled_joint_probs_M_K,\n",
    "                                               non_blocking=True),\n",
    "                             out=probs_b_M_C[i])\n",
    "            probs_b_M_C /= K\n",
    "            \n",
    "            q_1_M_1 = self.sampled_joint_probs_M_K.mean(dim=1, keepdim=True)[None]\n",
    "\n",
    "            output_entropies_B[start:end].copy_(torch.sum(\n",
    "                -torch.log(probs_b_M_C) * probs_b_M_C / q_1_M_1, dim=(1, 2)) / M,\n",
    "                                                non_blocking=True)\n",
    "            \n",
    "        return output_entropies_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.empty\" class=\"doc_header\"><code>SampledJointEntropy.empty</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.empty</code>(**`K`**:`int`, **`device`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.sample\" class=\"doc_header\"><code>SampledJointEntropy.sample</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.sample</code>(**`probs_N_K_C`**:`Tensor`, **`M`**:`int`)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.compute\" class=\"doc_header\"><code>SampledJointEntropy.compute</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.add_variables\" class=\"doc_header\"><code>SampledJointEntropy.add_variables</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.add_variables</code>(**`probs_N_K_C`**:`Tensor`, **`M2`**:`int`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.compute_batch\" class=\"doc_header\"><code>SampledJointEntropy.compute_batch</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.compute_batch</code>(**`probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SampledJointEntropy.empty)\n",
    "show_doc(SampledJointEntropy.sample)\n",
    "show_doc(SampledJointEntropy.compute)\n",
    "show_doc(SampledJointEntropy.add_variables)\n",
    "show_doc(SampledJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6493, dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4], 100000).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6520, dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4], 100000).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.9705, 5.6333, 5.9705, 5.6333, 5.9705, 5.6333, 5.9705, 5.6333],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.float)\n",
    "entropies =joint_entropy.add_variables(ys_ws[:4], 10000).compute_batch(ys_ws)\n",
    "assert np.allclose(entropies, [5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362], atol=0.01)\n",
    "entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamically choose the JointEntropy method\n",
    "\n",
    "Finally, we want to be able to dynamically pick either class depending on the maximum number of samples we want. (And also resample if necessary as we add variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class DynamicJointEntropy(JointEntropy):\n",
    "    inner: JointEntropy\n",
    "    probs_max_N_K_C: torch.Tensor\n",
    "    N: int\n",
    "    M: int\n",
    "        \n",
    "    def __init__(self, M:int, max_N: int, K: int, C: int, dtype=None, device=None):\n",
    "        self.M = M\n",
    "        self.N = 0\n",
    "        self.max_N = max_N\n",
    "        \n",
    "        self.inner = ExactJointEntropy.empty(K, dtype=dtype, device=device)\n",
    "        self.probs_max_N_K_C = torch.empty((max_N, K, C), dtype=dtype, device=device)\n",
    "    \n",
    "\n",
    "    def add_variables(self, probs_N_K_C: torch.Tensor) -> 'DynamicJointEntropy':\n",
    "        C = self.probs_max_N_K_C.shape[2]\n",
    "        new_N = probs_N_K_C.shape[0]\n",
    "        \n",
    "        assert self.probs_max_N_K_C.shape[0] >= self.N + new_N\n",
    "        assert self.probs_max_N_K_C.shape[2] == C\n",
    "        \n",
    "        self.probs_max_N_K_C[self.N:self.N+new_N] = probs_N_K_C\n",
    "        self.N += new_N       \n",
    "        \n",
    "        num_exact_samples = C**self.N\n",
    "        if num_exact_samples > self.M:\n",
    "            self.inner = SampledJointEntropy.sample(self.probs_max_N_K_C[:self.N], self.M)\n",
    "        else:\n",
    "            self.inner.add_variables(probs_N_K_C)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.inner.compute()\n",
    "    \n",
    "    def compute_batch(self,\n",
    "                      probs_B_K_C: torch.Tensor,\n",
    "                      output_entropies_B=None) -> torch.Tensor:\n",
    "        \"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\n",
    "        return self.inner.compute_batch(probs_B_K_C, output_entropies_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DynamicJointEntropy.add_variables\" class=\"doc_header\"><code>DynamicJointEntropy.add_variables</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DynamicJointEntropy.add_variables</code>(**`probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DynamicJointEntropy.compute_batch\" class=\"doc_header\"><code>DynamicJointEntropy.compute_batch</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DynamicJointEntropy.compute_batch</code>(**`probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DynamicJointEntropy.add_variables)\n",
    "show_doc(DynamicJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = DynamicJointEntropy(256, 8, K, 4, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4]).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(joint_entropy.inner) == ExactJointEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3331, dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy = joint_entropy.add_variables(ys_ws[4:]).compute()\n",
    "assert np.isclose(entropy, 9.2756, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(joint_entropy.inner) == SampledJointEntropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
